\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{enumitem}
\usepackage{tabu}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{tcolorbox} 
\usepackage{changepage} 
\usepackage{kpfonts}
\usepackage{picture}
\usepackage{venndiagram}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\field}{\mathcal{F}}
\newcommand{\prob}[1]{\mathbb{P}(#1)}
\newcommand{\probtext}[1]{\mathbb{P}(\text{#1})}

\renewcommand\labelitemi{---}


\newenvironment{nscenter}
 {\parskip=0pt\par\nopagebreak\centering}
 {\par\noindent\ignorespacesafterend}
 
\def\SPSB#1#2{\rlap{\textsuperscript{\textcolor{black}{#1}}}\SB{#2}}
 
\begin{document}
\title{%
  MATH 487: Probability Theory Exam II\\
  \large University of Nebraska-Lincoln, Fall 2017}
\author{Jacob Shiohira}
\maketitle

\newpage

% ================================================================================================================================
% ================================================================================================================================
\begin{center}
\section*{Random Variables}
\end{center}

\noindent
In general, a random variable is a symbolic representation (variable) of the outcome of experiment that depends on chance.

\vspace*{.5cm}
\subsection*{Discrete Random Variables}
\noindent
A \textbf{discrete random variable} has a countable number of outcomes. The distribution of probabilities of a discrete random variable are measured by a \textit{probability mass function}, $p_X(x)$, such that for any $x \in \text{Range}(X)$, 

\begin{equation*}
\prob{X = x} = p_X(x)
\end{equation*}

\noindent
where $p_X(x) > \geq 0$ and $\sum_{x} p_X(x) = 1$.

\subsection*{Continuous Random Variables}
\noindent
A \textbf{continuous random variable} has an infinitely uncountable number of outcomes. In other words, it can take on all values in a given interval. The distribution of probabilities of a continuous random variable are measured by a \textit{probability distribution function}, $f_X(x)$ such that for any $a,b \in \text{Range}(X)$,

\begin{equation*}
\prob{a \leq X \leq b} = \int_{a}^{b} f_X(x) dx
\end{equation*}

\noindent
where $f_X(x) > \geq 0$ and $\int_{- \infty}^{\infty} f_X(x) dx = 1$. Unlike a \textbf{probability mass function}, the probability of $f_X(x)$ such that $X = x$ is not defined in a \textbf{probability distribution function}.

\subsubsection*{PMF versus PDF}
\noindent
A \textbf{probability mass function} measures the total mass with the dimension of probability. A \textbf{probability distribution function} is essentially the measurement of probability on some continuum where $\rho (\frac{\text{probability}}{unit})$ is a non-constant density function $f_X(x)$.

\subsection*{Cumulative Distribution Functions}
A \textbf{cumulative distribution function}, or CDF, of a real-valued random variable $X$, or just distribution function of $X$, evaluated at $x$, is the probability that $X$ will take a value less than or equal to $x$. In other words, $\text{CDF}(x) = F_X(x)$ is equivalent to 

\begin{align*}
\prob{X \leq x} = \sum_{x_i \leq x} p_X(x) dx && \text{in the Discrete Case} \\
\prob{X \leq x} = \int_{- \infty}^{x} f_X(x) dx && \text{in the Continuous Case}
\end{align*}

\noindent
General properties of the CDF are as follows,

\begin{enumerate}
\item $F(x) = \prob{X \leq x}$,
\item $0 \leq F(x) \leq 1$,
\item $F(x)$ is a non-decreasing function. If $a < b$, then $F(a) < F(b)$,
\item $\lim_{x \rightarrow \infty} F(x) = 1$ and $\lim_{x \rightarrow - \infty} F(x) = 0$, 
\item $\prob{a \leq X \leq b} = F(b) - F(a)$,
\item $F'(x) = f_X(x)$.
\end{enumerate}

\begin{center}
\section*{Distributions}
\end{center}

\subsection*{Normal, Gaussian}
\noindent
Generally, the \textbf{normal} distribution is used $XXXX$. The probability distribution function is

\begin{align*}
N(\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi} \sigma} e^{- \frac{(x - \mu)^2}{2 \sigma^2}} \quad \text{ for } \sigma > 0.
\end{align*}

\noindent
while the cumulative distribution function is

\begin{equation*}
\iota(x) = \int_{- \infty}^{x} \frac{1}{\sqrt{2 \pi}} e^{- \frac{\mu^2}{2}} du.
\end{equation*}

\noindent
Thus, $\text{E}(X) = \mu$ and $\text{Var}(X) = \sigma^2$.

\subsection*{Cauchy}
\noindent
Generally, the \textbf{Cauchy} distribution is used for resonance behavior and used to measure outliers in data sets. The probability distribution function is

\begin{equation*}
C(\theta, \sigma) = \frac{1}{\pi \sigma} \Bigg [ \frac{1}{1 + \big ( \frac{x- \mu}{\sigma}\big )^2} \Bigg ].
\end{equation*}

\noindent
while the cumulative distribution function is

\begin{equation*}
F(X) = \frac{1}{\pi} \text{arctan}\big ( \frac{x - x_0}{\gamma}\big ) + \frac{1}{2}
\end{equation*}

\noindent
The $\text{E}(X)$ and the $\text{Var}(X)$ are not defined for the Cauchy distribution.

\subsection*{Exponential}
\noindent
Generally, the \textbf{Exponential} distribution is used to describe timing of events, such as the times between customer arrivals, times to equipment failure, etc. The probability distribution function is

\begin{equation*}
E(\lambda) = \lambda e^{- \lambda x}
\end{equation*}

\noindent
while the cumulative distribution function is

\[ F(x) =  \begin{cases} 
      1-e^{- \lambda x} & x \geq 0, \\
      0 & x < 0.
      \end{cases} \]

\noindent
Thus, $\text{E}(X) = \frac{1}{\lambda}$ and $\text{Var}(X) = \frac{1}{\lambda^2}$.

\subsection*{Gamma}
\noindent
Generally, the \textbf{Gamma} distribution is used to measure non-negative random variables such as rain fall amount, plant yields, time between earthquakes, etc. The probability distribution function is

\begin{equation*}
\Gamma ( \alpha, \beta) = \frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{- \lambda x}
\end{equation*}

\noindent
while the cumulative distribution function is

\begin{equation*}
F(x) = \frac{\Gamma_x(\gamma)} {\Gamma(\gamma)} \qquad x \geq 0; \gamma > 0
\end{equation*}

\noindent
Thus, $\text{E}(X) = \frac{\alpha}{\lambda}$ and $\text{Var}(X) = \frac{\alpha}{\lambda^2}$. Note that $\Gamma(n) = (n-1)!$.

\subsection*{Bernoulli}
\noindent
Generally, the \textbf{Bernoulli} distribution is used $XXXX$. The probability distribution function is

\[ f_X(x) =  \begin{cases} 
      \alpha & x = 1, \\
      1 - \alpha & x = 0.
      \end{cases} \]

\noindent
while the cumulative distribution function is

\[ F(x) =  \begin{cases} 
      0 & x \leq 0, \\
      1 - \alpha & 0 < x < 1, \\
      1 & x \geq 1.
      \end{cases} \]
      
\noindent
Thus, $\text{E}(X) = p$ and $\text{Var}(X) = p(1-p)$.

\subsection*{Binomial}
\noindent
Generally, the \textbf{Binomial} distribution is used $XXXXX$. The probability distribution function is

\begin{equation*}
b(n,p,m) = {n \choose m} p^n (1-p)^{n-m}
\end{equation*}

\noindent
while the cumulative distribution function is

\begin{equation*}
F(x;p,n) = \sum_{i = 0}^{x} {n \choose i} p^i (1- p)^{n - i}
\end{equation*}

\noindent
Thus, $\text{E}(X) = np$ and $\text{Var}(X) = npq$.

\subsection*{Geometric}
\noindent
Generally, the \textbf{Geometric} distribution is used $XXXXX$. The probability distribution function is

\begin{equation*}
g(p, k) = q^{k-1} p
\end{equation*}

\noindent
while the cumulative distribution function is

\begin{equation*}
F(x) = 1- (1 - p)^{k}
\end{equation*}

\noindent
Thus, $\text{E}(X) = \frac{1}{p}$ and $\text{Var}(X) = \frac{1-p}{p^2}$.

\subsection*{Poisson}
\noindent
Generally, the \textbf{Poisson} distribution is used $XXX$. The probability distribution function is

\begin{equation*}
P(k, \lambda) = e^{- \lambda} \frac{\lambda^k}{k!}.
\end{equation*}

\noindent
The $\text{E}(X) = \lambda$ and $\text{Var}(X) = \lambda$.

\begin{center}
\section*{Joint, Marginal, Conditional Functions}
\end{center}

\subsection*{Discrete Random Variables}
\noindent
Consider the two probability mass functions, $f_X(x) = \prob{X = x}$ and $f_Y(y) = \prob{Y = y}$. The \textbf{joint probability mass function} for $X,Y$ is

\begin{equation*}
\mathbb{P}_{XY}(x,y) = \prob{X = x \cap Y = y}.
\end{equation*}

\noindent
Note that 

\begin{equation*}
\sum_{x} \sum_{y} \mathbb{P}_{XY}(x,y) = \sum_{(x_i, y_j) \in R_{XY}} \mathbb{P}_{XY}(x_i,y_j) =  1,
\end{equation*}

\noindent
where $R_{XY} = \{ (x_i, y_j) \lvert x_i \in R_X, y_j \in R_Y \} = R_{X} \times R_{Y}$ where $R_X = {x_1, x_2, \ldots }$ and $R_Y = {y_1, y_2, \ldots }$. 

\vspace*{.5cm}
\noindent
The \textbf{marginal probability mass function} for any $x \in R_X$ is

\begin{equation*}
\mathbb{P}_X(x) = \prob{X = x} = \sum_{y_j \in R_Y} \prob{X = x, Y = y_j} = \sum_{y_j \in R_Y} \mathbb{P}_{XY}(x,y_j).
\end{equation*}

\noindent
Likewise, for any $y \in R_Y$,

\begin{equation*}
\mathbb{P}_Y(y) = \prob{Y = y} = \sum_{x_i \in R_X} \prob{X = x_i, Y = y} = \sum_{x_i \in R_X} \mathbb{P}_{XY}(x_i,y).
\end{equation*}

\vspace*{.5cm}
\noindent
The \textbf{conditional probability mass function} of $Y$ given $X = x$ is

\begin{align*}
f_{Y \lvert x} \frac{f_{XY}(x,y)}{f_X(x)} \qquad \text{for } f_X(x) > 0.
\end{align*}

\noindent
In other words, $f_{Y \lvert x}$ is equivalent to the joint probability of $X$ and $Y$ divided by the marginal probability for $X$. A few common properties of the \textbf{conditional probability mass function} are as follows,

\begin{enumerate}
\item $f_{Y \lvert x}(y) \geq 0$,
\item $\sum_{y} f_{Y \lvert x} = 1$,
\item $f_{Y \lvert x}(y) = \prob{Y = y \lvert X = x}$.
\end{enumerate}

\subsection*{Continuous Random Variables}
% Basically the same... copy from discrete random variables.

\subsection*{Law of Total Probability}
\begin{equation*}
\prob{x} = f_X(x) = \int_{- \infty}^{\infty} f_{X \lvert Y} (x, y) f_Y(y) dy.
\end{equation*}

\subsection*{Joint Cumulative Distribution Function}
\noindent
For two Random Variables $X,Y$, the joint Cumulative Distribution Function is

\begin{equation*}
F_{XY}(x,y) = \prob{X \leq x, Y \leq y} = \prob{ (X \leq x) \cap (Y \leq y)}. 
\end{equation*}

\subsection*{Marginal Cumulative Distribution Function}
\noindent
For any $x \in X$,

\begin{equation*}
F_X(x) = F_{XY}(x, \infty) = \lim_{y \rightarrow \infty} F_{XY}(x, y).
\end{equation*}

\noindent
For any $y \in Y$,

\begin{equation*}
F_Y(y) = F_{XY}(\infty, y) = \lim_{x \rightarrow \infty} F_{XY}(x, y).
\end{equation*}

\begin{center}
\item	\section*{Independent Random Variables}
\end{center}
Random variables $X,Y$ are independent \textit{if and only if} 

\begin{equation*}
\prob{X \in A, Y \in B} = \prob{X \in A} \prob{Y \in B}
\end{equation*}

\noindent
for any subsets $A,B \in \R^2$.
\subsection*{Expected Value of Random Variables}
\noindent
The expected value of a random variable is also its mean. So, $\text{E}(X) = \mu$. Common properties of the expected value of random variables are as follows,

\begin{enumerate}
\item $\text{E}(X + Y) = \text{E}(X) + \text{E}(Y)$,
\item $\text{E}(cX) = c\text{E}(X)$,
\item If $X,Y$ are independent, $\text{E}(XY) = \text{E}(X) \text{E}(Y)$.
\end{enumerate}

\subsubsection*{Discrete Random Variables}
\noindent
In the case of a discrete random variable, 

\begin{equation*}
\text{E}(X) = \sum_{k \in \Omega} k m(x)
\end{equation*}

\noindent
for distribution function $m(x)$.

\vspace*{.5cm}
\noindent
\textbf{Martingales} If $X$ is a random variable, $\Omega$ is a sample space, and $F_1, F_2, \ldots, F_r$ are events such that $F_i \cap F_j = \emptyset$ for $i \neq j$ and $\Omega = \bigcup_j F_j$, then

\begin{equation*}
\text{E}(X) = \sum_j \text{E}(X \lvert F_j) \prob{F_j}.
\end{equation*}

\subsubsection*{Continuous Random Variables}
% Basically the same... copy from discrete random variables.

\subsection*{Variance of Random Variables}
\noindent
Variance is how much the outcome of an experiment varies from the expected value, or mean, $\text{E}(X)$. Common properties of the variance of random variables are as follows,

\begin{enumerate}
\item $\text{Var}(X + b) = \text{Var}(X)$,
\item $\text{Var}(cX) = c^2 \text{E}(X)$,
\item If $X,Y$ are independent, $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$.
\end{enumerate}

\subsubsection*{Discrete Random Variables}
\noindent
Generally, for a distribution function $m(x)$,

\begin{equation*}
\text{Var}(X) = \text{E}((X-\mu)^2) = \sum_{x} (x - \mu)^2 m(x) = \text{E}(X^2) - \big [ \text{E}(X)^2 \big ].
\end{equation*}

\subsubsection*{Continuous Random Variables}
% Basically the same... copy from discrete random variables.

\begin{center}
\item \section*{Convolutions}
\end{center}
\end{document} 